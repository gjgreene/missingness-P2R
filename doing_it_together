### P2R checking done in colab 

!pip install pymc seaborn patsy miceforest openpyxl statsmodels NumPyro

# --- Step 1: Load and Profile Data ---
import pandas as pd
import numpy as np
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.metrics import log_loss
from sklearn.ensemble import ExtraTreesRegressor, ExtraTreesClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import HistGradientBoostingClassifier
import statsmodels.api as sm
import statsmodels.formula.api as smf
from scipy.stats import norm
import matplotlib.pyplot as plt
import seaborn as sns
import miceforest as mf
import itertools
from itertools import product
from scipy.stats import t
from scipy import stats
from sklearn.linear_model import BayesianRidge


# Load & rename
df = pd.read_excel("P2R_Full_Data3.xlsx")
df.rename(columns={
    "Type of Cancer": "Type_of_Cancer",
    "WIMD_Qunit":    "WIMD_Quint"
}, inplace=True)

# 1) Cast WIMD_Quint as an ordered categorical with levels 1→5
df["WIMD_Quint"] = pd.Categorical(
    df["WIMD_Quint"],
    categories=[1, 2, 3, 4, 5],
    ordered=True
)

# 2) Cast Sex as a nominal categorical with labels "Male" / "Female"
#    (Assuming Sex is coded 1=Male, 2=Female in your raw data)
df["Sex"] = pd.Categorical(
    df["Sex"].map({1: "Male", 2: "Female"}),
    categories=["Male", "Female"],
    ordered=False
)

# 3) Cast ASA_Grade as an ordered categorical with levels 1→4
df["ASA_Grade"] = pd.Categorical(
    df["ASA_Grade"],
    categories=[1, 2, 3, 4],
    ordered=True
)


df["Type_of_Cancer"] = pd.Categorical(
    df["Type_of_Cancer"].map({1: "Colorectal", 2: "Upper_GI", 3: "HPB"}),
    categories=["Colorectal", "Upper_GI", "HPB"],
    ordered=False
)


# 3) Cast Cancer_Stage  as an ordered categorical with levels 1→4
df["Cancer_Stage"] = pd.Categorical(
    df["Cancer_Stage"],
    categories=[1, 2, 3, 4],
    ordered=True
)


df["Chemotherapy"] = pd.Categorical(
    df["Chemotherapy"].map({0: "No", 1: "Yes"}),
    categories=["No", "Yes"],
    ordered=False
)

df["Radiotherapy"] = pd.Categorical(
    df["Radiotherapy"].map({0: "No", 1: "Yes"}),
    categories=["No", "Yes"],
    ordered=False
)

# Variable lists (adjust if yours differ)
treatment     = "Had_P2R"
outcome       = "Post_OP_LOS"
covariates    = [
    "ASA_Grade", "Cancer_Stage", "Type_of_Cancer", "WIMD_Quint",
    "Time_Diagnosis_Admission", "Age", "Sex", "Chemotherapy", "Radiotherapy"
]

ordinal_vars  = ["ASA_Grade", "Cancer_Stage"]
categoricals  = ["Sex", "Type_of_Cancer", "WIMD_Quint", "Chemotherapy", "Radiotherapy"]
continuous_vars = ["Age", "Time_Diagnosis_Admission", "TDA_sqrt"]


df_mice = df.copy()

# ───────────────────── Step 1: Cap extreme ages & create flags ─────────────────────
df_mice.loc[df_mice["Age"] > 100, "Age"] = np.nan
for col in ["Cancer_Stage", "Chemotherapy", "Radiotherapy"]:
    df_mice[f"{col}_imputed"] = df_mice[col].isnull().astype(int)

# ───────────────────── Step 2: Create Age² ─────────────────────
df_mice["Age_sq"] = df_mice["Age"] ** 2

# ───────────────────── Step 3: Cast categoricals/ordinals on the raw data ─────────────────────
ordinal_vars   = ["ASA_Grade", "Cancer_Stage"]
categoricals   = ["Sex", "Type_of_Cancer", "WIMD_Quint", "Chemotherapy", "Radiotherapy"]
continuous_vars = ["Age", "Age_sq", "Time_Diagnosis_Admission"]  # use raw TDA here

for col in ordinal_vars:
    df_mice[col] = df_mice[col].astype("category").cat.as_ordered()
df_mice[ordinal_vars + categoricals] = df_mice[ordinal_vars + categoricals].astype("category")

# ───────────────────── Step 4: Prepare the subset for MICE (no transform on TDA) ─────────────────────
data_for_mice = df_mice[
    ["Had_P2R", "Post_OP_LOS"] 
    + continuous_vars 
    + ordinal_vars 
    + categoricals
].copy()
data_for_mice.replace([np.inf, -np.inf], np.nan, inplace=True)

# Convert every categorical/ordinal into its integer code (−1 → NaN)
cols_to_code = [
    col for col in (ordinal_vars + categoricals)
    if col in data_for_mice.columns and pd.api.types.is_categorical_dtype(data_for_mice[col])
]
for col in cols_to_code:
    data_for_mice[col] = data_for_mice[col].cat.codes.replace({-1: np.nan}).astype(float)

n_observed = data_for_mice["Time_Diagnosis_Admission"].notna().sum()
print(f"Number of observed (non‐missing) Time_Diagnosis_Admission rows: {n_observed}")


### imputing on the GPU 

# 1) Prepare the numeric matrix, converting categoricals/ordinals to float codes
data_for_impute_orig = df[[*covariates, treatment, outcome]].copy()
data_for_impute_orig.replace([np.inf, -np.inf], np.nan, inplace=True)

for col in ["Cancer_Stage", "Chemotherapy", "Radiotherapy"]:
    # Check if the original column exists before creating the flag
    if col in df.columns:
        data_for_impute_orig[f"{col}_imputed"] = df[col].isnull().astype(int)
    else:
        print(f"Warning: Original column '{col}' not found in df. Cannot create imputation flag.")

# Store original category information before converting to codes
original_categories = {}
# The list of columns to process now includes the original categorical/ordinal columns
cols_to_process_for_codes = ordinal_vars + categoricals

for col in cols_to_process_for_codes:
    if col in data_for_impute_orig.columns and pd.api.types.is_categorical_dtype(data_for_impute_orig[col]):
        # Store category info *from the original df* to ensure correct mapping back
        if col in df.columns:
             original_categories[col] = (df[col].cat.categories, df[col].cat.ordered)
        else:
             # Handle case where the original column wasn't in df (shouldn't happen based on covariate list)
             print(f"Warning: Original column '{col}' not found for category mapping.")
             continue

        data_for_impute_orig[col] = data_for_impute_orig[col].cat.codes.replace(-1, np.nan).astype(float)

    elif col in data_for_impute_orig.columns and pd.api.types.is_numeric_dtype(data_for_impute_orig[col]):
        # Ensure numeric columns are float type for the imputer if they exist
         data_for_impute_orig[col] = data_for_impute_orig[col].astype(float)
    elif col not in data_for_impute_orig.columns:
        print(f"Warning: Column '{col}' not found in data_for_impute_orig. Skipping category processing.")


# ────────────────────────────────────────────────────────────────────────────────
# STEP A: RUN PMM *ONLY* ON TDA TO PRESERVE ITS OBSERVED DISTRIBUTION
# ────────────────────────────────────────────────────────────────────────────────
import miceforest as mf

# how many observed donors we have for TDA:
n_obs_tda = df["Time_Diagnosis_Admission"].notna().sum()

# define exactly the set of columns you want miceforest to see
# (covariates already *include* Time_Diagnosis_Admission)
features = covariates + [treatment, outcome]
# if you’re paranoid about duplicates, you can do:
# features = list(dict.fromkeys(features))

# Prepare the data for the TDA kernel. Drop rows with NaNs in the *predictor* columns.
# The predictor columns are 'features' excluding 'Time_Diagnosis_Admission'.
tda_predictor_cols = [col for col in features if col != "Time_Diagnosis_Admission"]

# Create a temporary DataFrame with only the necessary features for TDA imputation
data_for_tda_mice = df[features].copy()

# Drop rows that have NaNs in any of the predictor columns for TDA.
# Keep rows that have NaNs only in 'Time_Diagnosis_Admission' itself.
initial_rows = len(data_for_tda_mice)
# Check for NaNs in the columns that will be used as predictors by miceforest
# For TDA imputation, the predictors are all columns in `features` *except* TDA itself.
rows_to_drop_mask = data_for_tda_mice[tda_predictor_cols].isnull().any(axis=1)
data_for_tda_mice_cleaned = data_for_tda_mice[~rows_to_drop_mask].copy()

n_obs_tda_cleaned = data_for_tda_mice_cleaned["Time_Diagnosis_Admission"].notna().sum()
print(f"Number of observed Time_Diagnosis_Admission in cleaned data: {n_obs_tda_cleaned}")

tda_kernel = mf.ImputationKernel(
    # Use the cleaned data where predictors for TDA are complete
    data = data_for_tda_mice_cleaned.reset_index(drop=True),
    random_state = 42,
    num_datasets = 20,
    mean_match_candidates = {"Time_Diagnosis_Admission": int(n_obs_tda_cleaned)}, # Ensure it's an integer
)

# run the chained imputations
tda_kernel.mice(iterations=20)

# pull out all 20 completed datasets *from this TDA-only imputation*
# Note: These datasets only contain the columns passed to tda_kernel and have TDA imputed.
tda_imputations = [tda_kernel.complete_data(m)["Time_Diagnosis_Admission"] for m in range(20)]

# ────────────────────────────────────────────────────────────────────────────────

# ────────────────────────────────────────────────────────────────────────────────


from xgboost import XGBRegressor
class XGBWithNoise(XGBRegressor):
    def fit(self, X, y, **kwargs):
        super().fit(X, y, **kwargs)
        preds = super().predict(X)
        self._resid_std = np.std(y - preds, ddof=1)
        return self
    def predict(self, X, return_std=False):
        preds = super().predict(X)
        if return_std:
            return preds, np.full_like(preds, self._resid_std, dtype=float)
        return preds

from sklearn.impute import IterativeImputer

gpu_xgb = XGBWithNoise(
    tree_method='hist',
    device='cuda',
    random_state=42,
    n_estimators=100,
    max_depth=6,
)
imputer = IterativeImputer(
    estimator        = gpu_xgb,
    sample_posterior = True,
    max_iter         = 20,
    random_state     = 42,
)

# fit once on *all* columns (including TDA—even though we'll overwrite it)
imputer.fit(data_for_impute_orig.values)

# ────────────────────────────────────────────────────────────────────────────────
# STEP B: DRAW M COMPLETE SETS, INJECT PMM‐TDA, RECAST & RESTORE
# ────────────────────────────────────────────────────────────────────────────────
imputed_datasets = []
for m in range(20):
    imputer.random_state = 42 + m
    # The transformation now outputs an array with columns for the flags
    arr = imputer.transform(data_for_impute_orig.values)
    dfm = pd.DataFrame(arr,
                       columns=data_for_impute_orig.columns, # dfm now has the flag columns
                       index=data_for_impute_orig.index)

    # 1) “Inject” the PMM TDA draw
    # Ensure the index matches between dfm and tda_imputations
    dfm["Time_Diagnosis_Admission"] = tda_imputations[m].reindex(dfm.index)

    # 2) Map the float‐codes back to categories
    for col, (cats, ordered) in original_categories.items():
        if col in dfm.columns: # Check if the column exists in the imputed dataframe
             codes = dfm[col].round().astype(int).clip(0, len(cats)-1)
             dfm[col] = pd.Categorical.from_codes(codes, categories=cats, ordered=ordered)
        else:
             print(f"Warning: Imputed dataframe does not contain column '{col}' for category restoration.")


    # 3) Re‐attach treatment & outcome columns exactly as before

    if treatment in df.columns:
         dfm[treatment] = df[treatment].values
    else:
        print(f"Warning: Treatment column '{treatment}' not found in original df.")

    if outcome in df.columns:
        dfm[outcome]   = df[outcome].values
    else:
         print(f"Warning: Outcome column '{outcome}' not found in original df.")


    imputed_datasets.append(dfm)
    print(f"  → generated imputed dataset #{m+1}")

'''
## imputing on the CPU 
# ───────────────────── Step 6: Run MICE on raw TDA with maximal PMM ─────────────────────
vars_for_impute = ["Had_P2R", "Post_OP_LOS"] + continuous_vars + ordinal_vars + categoricals
n_observed_int = int(n_observed)
kernel = mf.ImputationKernel(
    data = data_for_mice[vars_for_impute],
    random_state = 42,
    mean_match_candidates = n_observed_int,  # <–– draw from all observed TDA values
    num_datasets = 20,
    save_all_iterations_data = True
)
kernel.mice(iterations = 50)

# ───────────────────── Step 7: Extract each completed dataset & restore categories ─────────────────────
imputed_datasets = []
for m in range(20):
    X_imp = kernel.complete_data(dataset = m).copy()

    # No back‐transform needed; TDA is already on raw scale
    X_imp["Time_Diagnosis_Admission"] = X_imp["Time_Diagnosis_Admission"].clip(lower=0)

    # Reconstruct categorical/ordinal columns from their integer codes:
    for col in ordinal_vars + categoricals:
        if col == "WIMD_Quint":
            orig_cats = pd.CategoricalDtype(categories=[1,2,3,4,5], ordered=True)
            codes = X_imp[col].astype(int).clip(0,4)
            X_imp[col] = pd.Categorical.from_codes(codes, categories=orig_cats.categories, ordered=True)

        elif col == "Sex":
            codes = X_imp[col].astype(int).clip(0,1)
            X_imp[col] = pd.Categorical.from_codes(
                codes,
                categories=["Male","Female"],
                ordered=False
            )

        elif col == "ASA_Grade":
            orig_cats = df["ASA_Grade"].cat.categories  # [1,2,3,4,5]
            codes = X_imp[col].astype(int).clip(0, len(orig_cats)-1)
            X_imp[col] = pd.Categorical.from_codes(codes, categories=orig_cats, ordered=True)

        else:
            orig_cats = df[col].cat.categories
            codes = X_imp[col].astype(int).clip(0, len(orig_cats)-1)
            X_imp[col] = pd.Categorical.from_codes(codes, categories=orig_cats, ordered=df[col].cat.ordered)

    # Clip Age and Age_sq back into plausible ranges:
    X_imp["Age"]   = X_imp["Age"].clip(16, 100)
    X_imp["Age_sq"] = X_imp["Age_sq"].clip(lower=0)

    # Replace any zero‐day LOS with random 0.5–1.5 as before
    X_imp["Post_OP_LOS"] = X_imp["Post_OP_LOS"].astype(float)
    mask_zero = X_imp["Post_OP_LOS"] == 0
    if mask_zero.any():
        np.random.seed(0)
        X_imp.loc[mask_zero, "Post_OP_LOS"] = np.random.uniform(0.5, 1.5, size=mask_zero.sum())

    # Re‐attach imputation flags:
    for col in ["Cancer_Stage", "Chemotherapy", "Radiotherapy"]:
      flag = f"{col}_imputed"
      if flag in df.columns:
        # copy the original 0/1 flag from the raw df
        X_imp[flag] = df[flag].values
      else:
        # if for some reason the flag didn’t exist, create a column of zeros
        X_imp[flag] = 0

    imputed_datasets.append(X_imp)

'''
# --- Step 2: Pooling “Table 1” with Rubin’s Rules (no "Missing" levels) ---

df_raw = df.copy()  # original, before imputation
imputed_dfs = imputed_datasets
treatment     = "Had_P2R"
outcome       = "Post_OP_LOS"
covariates    = [
    "ASA_Grade", "Cancer_Stage", "Type_of_Cancer", "WIMD_Quint",
    "Time_Diagnosis_Admission", "Age", "Sex", "Chemotherapy", "Radiotherapy"
]

ordinal_vars  = ["ASA_Grade", "Cancer_Stage"]
categoricals  = ["Sex", "Type_of_Cancer", "Chemotherapy", "Radiotherapy"]
continuous_vars = ["Time_Diagnosis_Admission", "Age"]

N = len(df_raw)
M = len(imputed_dfs)
continuous_vars_mean_sd = ["Age"] 
continuous_vars_median_iqr = ["Time_Diagnosis_Admission"]

def pooled_continuous_summary(var):
    """
    Pool means and standard deviations across imputations for a continuous variable
    using Rubin's Rules. Includes debugging prints.
    """
    print(f"\n--- Debugging {var} ---")
    # Collect each dataset’s non‐NaN values for `var`
    data_lists = []
    for i, ds in enumerate(imputed_datasets):
        series = ds[var].dropna()
        data_lists.append(series)
        print(f"Dataset {i}: N = {len(series)}, Mean = {series.mean():.4f} (approx)") # Added approx for potentially large numbers


    # Compute means
    means = np.array([s.mean() for s in data_lists])
    print(f"Means from each dataset: {means}")

    # Compute variances (within-dataset variance component)
    variances = []
    for s in data_lists:
        if len(s) <= 1:
            # Handle case with 0 or 1 non-NaN value - variance is 0
            variances.append(0.0)
            print("  Variance: 0.0 (N <= 1 or constant)")
        elif np.isclose(s.std(), 0): # Check if standard deviation is close to zero
             variances.append(0.0)
             print("  Variance: 0.0 (Values are effectively constant)")
        else:
            var_val = s.var(ddof=1)
            # Ensure variance is a finite number
            if np.isfinite(var_val):
                variances.append(var_val)
                print(f"  Variance (ddof=1): {var_val:.4f}")
            else:
                 variances.append(0.0) # Treat non-finite variance as 0
                 print(f"  Variance (ddof=1): Non-finite ({var_val}). Setting to 0.0")

    variances = np.array(variances)
    print(f"Variances from each dataset: {variances}")

    # Rubin’s rules
    mean_pooled = means.mean()
    within_var = variances.mean() # Average within-dataset variance

    # Between-dataset variance (variance of the means)
    if M > 1:
        between_var = means.var(ddof=1)
    else:
        # Cannot calculate variance of means with only 1 dataset
        between_var = 0.0
    print(f"Pooled Mean: {mean_pooled:.4f}")
    print(f"Average Within-Variance: {within_var:.4f}")
    print(f"Between-Variance (of means): {between_var:.4f}")


    # Total variance
    total_var = within_var + (1 + 1/M) * between_var
    print(f"Total Variance: {total_var:.4f}")

    # Pooled standard deviation
    # Ensure total_var is not negative before taking sqrt (shouldn't happen theoretically, but good practice)
    if total_var >= 0:
        sd_pooled = np.sqrt(total_var)
        print(f"Pooled SD: {sd_pooled:.4f}")
    else:
        sd_pooled = np.nan # Indicate an issue if total_var is negative
        print(f"WARNING: Total Variance is negative ({total_var}). Pooled SD is NaN.")

    print("--- End Debugging ---")
    return mean_pooled, sd_pooled

def pooled_categorical_summary(var):
    """
    Pool counts and proportions across imputations for each category of `var`,
    ensuring integer counts sum exactly to N.
    """
    # Identify levels from first imputation
    ds0 = imputed_datasets[0]
    levels = list(ds0[var].cat.categories)
    
    # Build matrix of proportions
    prop_matrix = np.zeros((M, len(levels)))
    for i, ds in enumerate(imputed_datasets):
        counts = ds[var].value_counts()
        for j, lvl in enumerate(levels):
            prop_matrix[i, j] = counts.get(lvl, 0) / len(ds)
    
    # Average proportions
    prop_pooled = prop_matrix.mean(axis=0)  # length = number of levels
    raw_counts = prop_pooled * N
    
    # Floor and distribute remainder for integer counts
    floored = np.floor(raw_counts).astype(int)
    remainder = N - floored.sum()
    frac_parts = raw_counts - floored
    if remainder > 0:
        largest_idx = np.argsort(frac_parts)[-remainder:]
    else:
        largest_idx = []
    final_counts = floored.copy()
    for idx in largest_idx:
        final_counts[idx] += 1
    
    result = {}
    for j, lvl in enumerate(levels):
        result[lvl] = (final_counts[j], prop_pooled[j])
    return result

def pooled_continuous_summary_median_iqr(var):
    """
    Pool medians and interquartile ranges across imputations for a continuous variable.
    Calculates median, 25th, and 75th percentiles for each dataset and averages them.
    """
    print(f"\n--- Debugging {var} Median/IQR ---")
    medians = []
    q1s = [] # 25th percentile
    q3s = [] # 75th percentile

    for i, ds in enumerate(imputed_datasets):
        series = ds[var].dropna()
        if len(series) > 0:
            medians.append(series.median())
            q1s.append(series.quantile(0.25))
            q3s.append(series.quantile(0.75))
            print(f"Dataset {i}: Median = {medians[-1]:.4f}, Q1 = {q1s[-1]:.4f}, Q3 = {q3s[-1]:.4f}")
        else:
            # Handle empty series case
            medians.append(np.nan)
            q1s.append(np.nan)
            q3s.append(np.nan)
            print(f"Dataset {i}: Empty series or all NaNs.")


    # Pool by averaging the quantiles
    pooled_median = np.nanmean(medians)
    pooled_q1 = np.nanmean(q1s)
    pooled_q3 = np.nanmean(q3s)
    pooled_iqr = pooled_q3 - pooled_q1

    print(f"Pooled Median: {pooled_median:.4f}")
    print(f"Pooled Q1: {pooled_q1:.4f}")
    print(f"Pooled Q3: {pooled_q3:.4f}")
    print(f"Pooled IQR: {pooled_iqr:.4f}")
    print("--- End Debugging Median/IQR ---")

    return pooled_median, pooled_q1, pooled_q3, pooled_iqr

# Build pooled “Table 1”
rows = []
for var in covariates:
    # Check if the variable exists in the imputed datasets
    if var not in imputed_datasets[0].columns:
        print(f"WARNING: Variable '{var}' not found in imputed datasets. Skipping.")
        continue

    miss_pct = df_raw[var].isna().mean() * 100

    if var in continuous_vars_mean_sd:
        # Use the Mean/SD pooling function for these variables
        mean_p, sd_p = pooled_continuous_summary(var) # Use the debug-enhanced function
        sd_str = f"{sd_p:.1f}" if np.isfinite(sd_p) else "NaN"
        rows.append({
            "Variable": var,
            "Level": "",
            "Pooled N": N, # Total N for continuous vars
            "Pooled (%)": f"{mean_p:.1f} ± {sd_str}",
            "% Missing": f"{miss_pct:.1f}"
        })
    elif var in continuous_vars_median_iqr:
         # Use the Median/IQR pooling function for these variables
        median_p, q1_p, q3_p, iqr_p = pooled_continuous_summary_median_iqr(var)
        # Format as Median (IQR)
        median_iqr_str = f"{median_p:.1f} ({iqr_p:.1f})" if np.isfinite(median_p) and np.isfinite(iqr_p) else "NaN (NaN)"
        rows.append({
            "Variable": var,
            "Level": "",
            "Pooled N": N, # Total N for continuous vars
            "Pooled (%)": median_iqr_str,
            "% Missing": f"{miss_pct:.1f}"
        })
    else: # This will be for categorical variables
        # Use the existing categorical summary function
        pooled = pooled_categorical_summary(var)
        for lvl, (cnt, prop) in pooled.items():
            rows.append({
                "Variable": var,
                "Level": str(lvl),
                "Pooled N": f"{cnt:,d}",
                "Pooled (%)": f"{prop*100:.1f}%",
                "% Missing": f"{miss_pct:.1f}"
            })

table1 = pd.DataFrame(rows, columns=["Variable", "Level", "Pooled N", "Pooled (%)", "% Missing"])
display(table1)
table1.to_excel("pooled_table1.xlsx", index=False)


# 1) Put them in a list for easy looping
imputed_dfs = [ kernel.complete_data(dataset=i).copy()
                for i in range(m) ]

# 2) Define covariates and hyperparameter grid for logistic‐PS model
ps_covariates = [
    "Age",
    # For categorical variables, we’ll one‐hot encode later below:
    "Time_Diagnosis_Admission",
    "Sex", "Type_of_Cancer", "ASA_Grade", "Cancer_Stage", "WIMD_Quint",
    "Chemotherapy", "Radiotherapy",
    # We also include the “_imputed” flags as predictors:
    "Cancer_Stage_imputed", "Chemotherapy_imputed", "Radiotherapy_imputed"
]

# Define a small grid over C and penalty for LogisticRegression
# Feel free to expand or modify this list as needed.
C_grid       = [0.01, 0.1, 1.0, 10.0]
penalty_grid = ["l2"]       # you can add "l1" if you install a solver that supports it (e.g. "liblinear")
solver_map   = {"l2": "saga"}  # for 'l1' you might choose 'liblinear' or 'saga'

param_grid = list(product(C_grid, penalty_grid))

# 3) Helper functions: (a) Build design matrix with one‐hot encoding (b) Compute stabilized weights (c) Compute SMDs
def build_design_matrix(df, covariate_list):
    """
    From df, take all covariates in covariate_list, one-hot encode categoricals,
    and return X (DataFrame) and feature_names (list).
    """
    # We know that "Sex", "Type_of_Cancer", "ASA_Grade", "Cancer_Stage", "WIMD_Quint",
    # "Chemotherapy", "Radiotherapy" are categorical/ordinal; the rest numeric.
    df2 = df[covariate_list].copy()

    # One‐hot encode all columns that pandas sees as category dtype or object dtype
    cat_cols = [c for c in covariate_list if pd.api.types.is_categorical_dtype(df2[c]) or df2[c].dtype == object]

    X = pd.get_dummies(df2, columns=cat_cols, drop_first=True, dtype=float)
    return X, list(X.columns)

def stabilized_weights(ps_array, treatment_array):
    """
    Given an array of propensity scores (prob. of Had_P2R=1) and a binary treatment_series,
    returns stabilized IPT weights (clipped at 1st and 99th percentiles).
    """
    t = treatment_array.values
    p = ps_array
    p_t = t.mean()
    w = np.where(t == 1, (p_t / p), ((1 - p_t) / (1 - p)))
    # Trim at 1st and 99th percentiles
    lo, hi = np.percentile(w, [1, 99])
    w_clip = np.clip(w, lo, hi)
    return w_clip

def standardised_mean_diff_binary(x_series, t_series, w_series=None):
    """
    Compute SMD for a binary indicator x_series (0/1) between treated (t=1) and control (t=0).
    If w_series is provided, use a weighted mean/variance; else simple.
    """
    x = x_series.values
    t = t_series.values
    if w_series is None:
        m1 = x[t == 1].mean()
        m0 = x[t == 0].mean()
        v1 = x[t == 1].var(ddof=1)
        v0 = x[t == 0].var(ddof=1)
    else:
        w = w_series.values
        w1 = w[t == 1]
        w0 = w[t == 0]
        x1 = x[t == 1]
        x0 = x[t == 0]
        # avoid division by zero
        if w1.sum() == 0:
            return np.nan
        if w0.sum() == 0:
            return np.nan
        m1 = np.average(x1, weights=w1)
        m0 = np.average(x0, weights=w0)
        v1 = np.average((x1 - m1) ** 2, weights=w1)
        v0 = np.average((x0 - m0) ** 2, weights=w0)
    denom = np.sqrt((v1 + v0) / 2)
    if denom < 1e-8:
        return 0.0
    return abs(m1 - m0) / denom

def standardised_mean_diff_continuous(x_series, t_series, w_series=None):
    """
    Compute SMD for a continuous x_series between treated (t=1) and control (t=0).
    If w_series is provided, use weighted means/vars; else simple.
    """
    x = x_series.values
    t = t_series.values
    if w_series is None:
        x1 = x[t == 1]
        x0 = x[t == 0]
        if len(x1) < 2 or len(x0) < 2:
            return np.nan
        m1 = x1.mean()
        m0 = x0.mean()
        v1 = x1.var(ddof=1)
        v0 = x0.var(ddof=1)
    else:
        w = w_series.values
        x1 = x[t == 1]
        x0 = x[t == 0]
        w1 = w[t == 1]
        w0 = w[t == 0]
        if w1.sum() < 1e-8 or w0.sum() < 1e-8:
            return np.nan
        m1 = np.average(x1, weights=w1)
        m0 = np.average(x0, weights=w0)
        v1 = np.average((x1 - m1) ** 2, weights=w1)
        v0 = np.average((x0 - m0) ** 2, weights=w0)
    denom = np.sqrt((v1 + v0) / 2)
    if denom < 1e-8:
        return 0.0
    return abs(m1 - m0) / denom

def calculate_smds(df, treatment_col, weight_col, covariate_list):
    """
    Returns a dict of SMDs keyed by “covariate” (for continuous) or “cov=level” (for categorical),
    both before and after weighting.  Actually returns a tuple:
       (smd_dict_unweighted, smd_dict_weighted)
    """
    t = df[treatment_col]
    w = df[weight_col]
    smd_unw = {}
    smd_w   = {}

    for cov in covariate_list:
        if pd.api.types.is_numeric_dtype(df[cov]):
            # continuous
            smd_unw[cov] = standardised_mean_diff_continuous(df[cov], t, None)
            smd_w[cov]   = standardised_mean_diff_continuous(df[cov], t, w)
        else:
            # categorical/ordinal: compute SMD for each level
            levels = df[cov].cat.categories if pd.api.types.is_categorical_dtype(df[cov]) else df[cov].unique()
            for lvl in levels:
                mask = (df[cov] == lvl).astype(int)
                key = f"{cov}={lvl}"
                smd_unw[key] = standardised_mean_diff_binary(mask, t, None)
                smd_w[key]   = standardised_mean_diff_binary(mask, t, w)
    return smd_unw, smd_w

# ─────────────────────────────────────────────────────────────────────────────────────────────
# 4) Now loop over each imputed dataset, tune PS, compute weights, fit NB, store results
results_per_imputation = []  # Will hold dicts with best‐params, PS‐weights, NB‐coef, NB‐var, etc.

for idx, df_imp in enumerate(imputed_dfs):
    print(f"\n──── Processing imputed dataset {idx} ────")

    # 4.1) Build design matrix for PS
    X_ps, ps_feature_names = build_design_matrix(df_imp, ps_covariates)
    t_series = df_imp["Had_P2R"]

    # 4.2) Scale numeric columns for logistic regression
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_ps.fillna(X_ps.median()))

    best_score = np.inf
    best_params = None
    best_ps_model = None
    best_ps      = None

    # 4.3) Grid‐search over (C, penalty)
    for C_val, pen in param_grid:
        solver = solver_map[pen]
        try:
            lr = LogisticRegression(
                C = C_val,
                penalty = pen,
                solver = solver,
                max_iter = 5000,
                n_jobs = -1
            )
            # Fit on scaled design matrix:
            lr.fit(X_scaled, t_series)
            ps_pred = lr.predict_proba(X_scaled)[:, 1]

            # Evaluate via log‐loss (lower = better).  You could also use BIC/AIC from a statsmodels fit,
            # but log‐loss is a straightforward metric here.
            loss = log_loss(t_series, ps_pred, labels=[0,1])

            if loss < best_score:
                best_score = loss
                best_params = (C_val, pen, solver)
                best_ps_model = lr
                best_ps = ps_pred
        except Exception:
            continue

    print(f"  Best PS hyperparams: C={best_params[0]}, penalty={best_params[1]}, solver={best_params[2]}")
    print(f"  Best PS log‐loss = {best_score:.4f}")

    # 4.4) Compute stabilized weights for this imputed set
    w_imp = stabilized_weights(best_ps, t_series)
    df_imp["iptw"] = w_imp

    # 4.5) Check balance (compute SMDs before/after)
    smd_unw, smd_w = calculate_smds(df_imp, "Had_P2R", "iptw", ps_covariates)

    # (Optionally: you could save a “love plot” image here, but not shown.)

    # 4.6) Fit weighted negative‐binomial on LOS using statsmodels
    #      We need a formula that includes all covariates (categorical as C(...))
    cat_terms = [f"C({c})" for c in ["Sex", "Type_of_Cancer", "ASA_Grade", "Cancer_Stage", "WIMD_Quint", "Chemotherapy", "Radiotherapy"]]
    cont_terms = ["Age", "Time_Diagnosis_Admission", "Cancer_Stage_imputed", "Chemotherapy_imputed", "Radiotherapy_imputed"]
    formula = (
      "Post_OP_LOS ~ Had_P2R"
      + " + C(ASA_Grade, Treatment(reference=2))"
      + " + C(Cancer_Stage, Treatment(reference=2))"
      + " + C(Type_of_Cancer)"
      + " + C(WIMD_Quint)"
      + " + C(Sex)"
      + " + C(Chemotherapy)"
      + " + C(Radiotherapy)"
      + " + Age"
      + " + Time_Diagnosis_Admission"
)

    # Statsmodels wants a DataFrame with no missing; our imputations have filled TDA and Age as needed
    df_model = df_imp.copy()
    df_model["weights"] = w_imp

    # Sweep alpha (dispersion) to pick the best one via log‐likelihood
    alphas = np.linspace(0.01, 5, 100)
    best_llf = -np.inf
    best_alpha = None

    for alpha in alphas:
        try:
            tmp = smf.glm(
                formula = formula,
                data = df_model,
                family = sm.families.NegativeBinomial(alpha = alpha),
                freq_weights = df_model["weights"]
            ).fit(disp=False)
            if tmp.llf > best_llf:
                best_llf = tmp.llf
                best_alpha = alpha
        except Exception:
            continue

    # Refit the NB model with best_alpha
    nb_model = smf.glm(
        formula = formula,
        data = df_model,
        family = sm.families.NegativeBinomial(alpha = best_alpha),
        freq_weights = df_model["weights"]
    ).fit()

    # Extract coefficient estimates and robust‐covariance (so we have SEs)
    coef_table = nb_model.summary2().tables[1].copy()
    #   ["Coef.", "Std.Err.", "z", "P>|z|", ...]
    # We only need "Coef." and "Std.Err." for pooling.

    coefs = coef_table["Coef."].to_dict()
    ses   = coef_table["Std.Err."].to_dict()

    # 4.7) Save everything into a dict for this imputation
    results_per_imputation.append({
        "imputation_index": idx,
        "best_ps_params": best_params,
        "best_ps_loss": best_score,
        "smd_unweighted": smd_unw,
        "smd_weighted": smd_w,
        "nb_coefs": coefs,
        "nb_ses"  : ses,
        "nb_df_resid": nb_model.df_resid,      # for pooling degrees of freedom
    })
glm_names = nb_model.model.exog_names  # e.g. ['Intercept','C(ASA_Grade)[T.3]', …]
mle_vals  = nb_model.params[glm_names].values
# ─────────────────────────────────────────────────────────────────────────────────────────────
# 5) Now pool NB coefficients via Rubin’s rules
all_param_names = sorted({
    name
    for res in results_per_imputation
    for name in res["nb_coefs"].keys()
})

m = len(results_per_imputation)

# Prepare storage for each parameter’s (Q_i, U_i)
Q_m = {name: [] for name in all_param_names}
U_m = {name: [] for name in all_param_names}

for res in results_per_imputation:
    for name in all_param_names:
        beta_i = res["nb_coefs"].get(name, 0.0)
        se_i   = res["nb_ses"].get(name, 0.0)
        Q_m[name].append(beta_i)
        U_m[name].append(se_i**2)

pooled_results = []
for name in all_param_names:
    Q_list = np.array(Q_m[name])    # M×1 array of coefficients
    U_list = np.array(U_m[name])    # M×1 array of variances

    Q_bar = Q_list.mean()           # pooled point estimate
    U_bar = U_list.mean()           # average within‐imputation variance
    B     = Q_list.var(ddof=1)      # between‐imputation variance

    # Total variance per Rubin’s rule:
    T_var = U_bar + (1 + 1/m) * B
    T_se  = np.sqrt(T_var)

    # Compute Rubin’s df (requires each imputation’s residual df; pick from the first fit)
    df_obs   = results_per_imputation[0]["nb_df_resid"]
    if B < 1e-12:
        df_old = np.inf
    else:
        df_old = (m - 1) * (1 + (U_bar / ((1 + 1/m) * B)))**2
    df_rubin = (df_old * df_obs) / (df_old + df_obs)

    # 95% CI bounds on the log‐scale:
    t_crit = stats.t.ppf(0.975, df_rubin)
    CI_lo  = Q_bar - t_crit * T_se
    CI_hi  = Q_bar + t_crit * T_se

    pooled_results.append({
        "parameter":    name,
        "pooled_coef":  Q_bar,
        "pooled_se":    T_se,
        "df_rubin":     df_rubin,
        "95%_CI_lower": CI_lo,
        "95%_CI_upper": CI_hi,
        "within_var":   U_bar,
        "between_var":  B
    })

pooled_df = pd.DataFrame(pooled_results).sort_values("parameter").reset_index(drop=True)

# Now add p‐values and IRRs:

from scipy import stats

pooled_df["t_value"] = pooled_df["pooled_coef"] / pooled_df["pooled_se"]
pooled_df["p_value"] = 2 * (1 - stats.t.cdf(
    np.abs(pooled_df["t_value"]), df=pooled_df["df_rubin"]))

# Exponentiate to get IRR on days‐scale
pooled_df["IRR"]         = np.exp(pooled_df["pooled_coef"])
pooled_df["IRR_lower95"] = np.exp(pooled_df["95%_CI_lower"])
pooled_df["IRR_upper95"] = np.exp(pooled_df["95%_CI_upper"])

# Re‐order columns for easier reading:
col_order = [
    "parameter", "pooled_coef", "pooled_se", "t_value", "p_value",
    "IRR", "IRR_lower95", "IRR_upper95",
    "95%_CI_lower", "95%_CI_upper", "df_rubin",
    "within_var", "between_var"
]
pooled_df = pooled_df[col_order]

pd.set_option("display.float_format", lambda x: f"{x:.4f}")
print(pooled_df)

pooled_df.to_excel("pooled_nb_model_results.xlsx", index=False)

# ────────────────────────────────────────────────────────────────────────────────
df_imp0 = imputed_datasets[0].copy()         
w_imp0  = df_imp0["iptw"].values  

# 2b) Choose exactly which covariates to check balance on (must match your PS model’s covariates):
ps_covariates = [
    "Age", "Time_Diagnosis_Admission",
    "Sex", "Type_of_Cancer", "ASA_Grade", "Cancer_Stage", "WIMD_Quint",
    "Chemotherapy", "Radiotherapy",
    "Cancer_Stage_imputed", "Chemotherapy_imputed", "Radiotherapy_imputed"
]
# Make sure the column list matches exactly what’s in df_imp0:
ps_covariates = [c for c in ps_covariates if c in df_imp0.columns]
# Build X (one‐hot encoded) and feature names:
X_ps, feature_names = build_design_matrix(df_imp0, ps_covariates)

# Extract treatment vector:
treat = df_imp0["Had_P2R"].copy()

# Fit a logistic regression for PS (un‐weighted, no tuning here as an example):
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(
    penalty="l2", solver="liblinear", max_iter=5000,
    C=1.0  # you can later loop over a C‐grid if you want hyper‐tuning
)
lr.fit(X_ps.values, treat.values)
ps_array = lr.predict_proba(X_ps.values)[:, 1]  # (n_samples,)

# Compute stabilized IPT weights and attach to df_imp0:
df_imp0["iptw"] = stabilized_weights(ps_array, treat)

smd_unw0, smd_w0 = calculate_smds(
    df = df_imp0,
    treatment_col = "Had_P2R",
    weight_col    = "iptw",
    covariate_list = ps_covariates
)

# Wrap those into a DataFrame that we can plot:
love_df = pd.DataFrame({
    "Unweighted": pd.Series(smd_unw0),
    "Weighted":   pd.Series(smd_w0)
})

# Sort by the unweighted SMD so that the Love plot orders covariates by baseline imbalance:
love_df = love_df.sort_values("Unweighted", ascending=True)

# ────────────────────────────────────────────────────────────────────────────────
# Step B2: Draw the Love plot and save as PNG
# ────────────────────────────────────────────────────────────────────────────────

import matplotlib.pyplot as plt

plt.figure(figsize=(8, max(4, 0.3 * len(love_df))))
ax = love_df.plot.barh(
    figsize=(8, max(4, 0.3 * len(love_df))),
    legend=True,
    color=["#80808080", "#2E86AB"]  # (light gray for unweighted, blue for weighted)
)
ax.axvline(0.10, color="gray", linestyle="--", linewidth=1, label="SMD = 0.10")
ax.axvline(0.20, color="red", linestyle="--", linewidth=1, label="SMD = 0.20")
ax.set_xlabel("Standardized Mean Difference")
ax.set_title("Love Plot: Covariate Balance (Imputation #0)")
ax.legend(loc="lower right")
plt.tight_layout()
plt.savefig("love_plot_imputation0.png", dpi=300)
plt.show()

print("→ Saved Love plot as 'love_plot_imputation0.png'")


#_______________________________________
# love plot per imputed dataset

for i, df_imp in enumerate(imputed_datasets):
    X_ps, _ = build_design_matrix(df_imp, ps_covariates)
    treat = df_imp["Had_P2R"]
    lr.fit(X_ps.values, treat.values)
    ps_array = lr.predict_proba(X_ps.values)[:,1]
    df_imp["iptw"] = stabilized_weights(ps_array, treat)

    smd_unw, smd_w = calculate_smds(df_imp, "Had_P2R", "iptw", ps_covariates)
    love_df = pd.DataFrame({"Unweighted": pd.Series(smd_unw), "Weighted": pd.Series(smd_w)})
    love_df = love_df.sort_values("Unweighted", ascending=True)

    plt.figure(figsize=(8, max(4, 0.3*len(love_df))))
    ax = love_df.plot.barh(figsize=(8, max(4, 0.3*len(love_df))))
    ax.axvline(0.10, color="gray", linestyle="--", linewidth=1)
    ax.axvline(0.20, color="red", linestyle="--", linewidth=1)
    ax.set_xlabel("SMD")
    ax.set_title(f"Love Plot: Imputation #{i}")
    plt.tight_layout()
    filename = f"love_plot_imputation{i}.png"
    plt.savefig(filename, dpi=300)
    plt.close()
    print(f"→ Saved {filename}")


#_______________________________________________
# love plot pooled across imputations

ps_covariates = [
    "Age", "Time_Diagnosis_Admission",
    "Sex", "Type_of_Cancer", "ASA_Grade", "Cancer_Stage", "WIMD_Quint",
    "Chemotherapy", "Radiotherapy",
    "Cancer_Stage_imputed", "Chemotherapy_imputed", "Radiotherapy_imputed"
]

# Filter out any covariate not present in all imputed datasets (defensive check):
ps_covariates = [c for c in ps_covariates if all(c in df.columns for df in imputed_datasets)]

# Each element of these lists will be a dict: { covariate (or "cov=level"): SMD_value }
smd_unweighted_list = []
smd_weighted_list   = []

for df_imp in imputed_datasets:
    # Ensure “iptw” is present in df_imp: it should already be computed before pooling
    if "iptw" not in df_imp.columns:
        raise RuntimeError("Each imputed dataframe must have an 'iptw' column before pooling.")

    smd_unw, smd_w = calculate_smds(
        df        = df_imp,
        treatment_col = "Had_P2R",
        weight_col    = "iptw",
        covariate_list = ps_covariates
    )
    smd_unweighted_list.append(smd_unw)
    smd_weighted_list.append(smd_w)


# ────────────────────────────────────────────────────────────────────────────────
# 2) Pool (i.e. average) each SMD across imputations
# ────────────────────────────────────────────────────────────────────────────────

# First gather the union of all keys (covariate‐names or “cov=level”) across imputations:
all_smd_keys = sorted({
    key
    for smd_dict in smd_unweighted_list
    for key in smd_dict.keys()
})

# Build a DataFrame of shape (n_imputations × n_keys) for unweighted SMDs
df_unw = pd.DataFrame(
    { key: [d.get(key, np.nan) for d in smd_unweighted_list]
      for key in all_smd_keys }
)

# Similarly for weighted SMDs
df_w = pd.DataFrame(
    { key: [d.get(key, np.nan) for d in smd_weighted_list]
      for key in all_smd_keys }
)

# Now compute the average (mean) across rows (axis=0), ignoring NaNs
pooled_unw = df_unw.mean(axis=0, skipna=True)
pooled_w   = df_w.mean(axis=0, skipna=True)

# Put them into a single DataFrame, sorted by pooled_unweighted:
pooled_love = pd.DataFrame({
    "Unweighted": pooled_unw,
    "Weighted":   pooled_w
})
pooled_love = pooled_love.sort_values("Unweighted", ascending=True)


# ────────────────────────────────────────────────────────────────────────────────
# 3) Draw & save the “pooled” Love plot
# ────────────────────────────────────────────────────────────────────────────────

plt.figure(figsize=(8, max(4, 0.3 * len(pooled_love))))
ax = pooled_love.plot.barh(
    figsize=(8, max(4, 0.3 * len(pooled_love))),
    color=["#99999980", "#2E86AB"],  # gray for unweighted, blue for weighted
    legend=True
)
ax.axvline(0.10, color="gray", linestyle="--", linewidth=1, label="SMD = 0.10")
ax.axvline(0.20, color="red", linestyle="--", linewidth=1, label="SMD = 0.20")
ax.set_xlabel("Pooled Standardized Mean Difference")
ax.set_title("Pooled Love Plot")
ax.legend(loc="lower right")
plt.tight_layout()
plt.savefig("love_plot_pooled.png", dpi=300)
plt.show()

print("→ Saved pooled Love plot as 'love_plot_pooled.png'")




###____________________________________________________________________
### Influence Diagnostics (Deviance Residuals vs Fitted Values)
###____________________________________________________________________

# Extract fitted values and deviance residuals:
df_model["fitted"] = nb_model.fittedvalues
df_model["resid_deviance"] = nb_model.resid_deviance

# Identify the top 5 most extreme deviance residuals (by absolute value):
extreme_idx = np.abs(df_model["resid_deviance"]).argsort()[-5:]
extreme_points = df_model.loc[extreme_idx]

# Plotting:
plt.figure(figsize=(10, 5))
plt.scatter(df_model["fitted"], df_model["resid_deviance"],
            alpha=0.6, s=20, color="#2E86AB")
plt.axhline(0, linestyle="--", color="grey", linewidth=1)
plt.axhline(2, linestyle="--", color="red", alpha=0.6, linewidth=1)
plt.axhline(-2, linestyle="--", color="red", alpha=0.6, linewidth=1)

for idx, row in extreme_points.iterrows():
    plt.annotate(
        idx, 
        (row["fitted"], row["resid_deviance"]),
        fontsize=8,
        alpha=0.8
    )

plt.title("Deviance Residuals vs Fitted Values")
plt.xlabel("Fitted Values (NB)")
plt.ylabel("Deviance Residuals")
plt.tight_layout()
plt.show()


# ────────────────────────────────────────────────────────────────────────────────
# Step 8: Bayesian Negative-Binomial via PyMC
# ────────────────────────────────────────────────────────────────────────────────

import pymc as pm
import arviz as az
import patsy
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Choose one of your imputed datasets (we’ll use the first):
# Ensure df0 is defined before this block if it wasn't in the previous cells
# If running this block standalone, you'd need to re-run the imputation steps or load df0
if 'imputed_datasets' not in globals() or not imputed_datasets:
     raise RuntimeError("Imputed datasets not found. Please run imputation steps first.")
df0 = imputed_datasets[0]


# Re-use the same RHS you used in statsmodels:
# This formula includes the treatment variable and all covariates
patsy_expr = (
    "Had_P2R" # Treatment variable
    + " + C(ASA_Grade, Treatment(reference=2))" # Categorical covariates with reference levels
    + " + C(Cancer_Stage, Treatment(reference=2))"
    + " + C(Type_of_Cancer)"
    + " + C(WIMD_Quint)"
    + " + C(Sex)"
    + " + C(Chemotherapy)"
    + " + C(Radiotherapy)"
    + " + Age" # Continuous covariates
    + " + Time_Diagnosis_Admission"
    # Add imputation flags if you want them in the Bayesian model too
    + " + Cancer_Stage_imputed"
    + " + Chemotherapy_imputed"
    + " + Radiotherapy_imputed"
)


# Build the design matrix (keep as DataFrame for legible column names)
X_bayes = patsy.dmatrix(patsy_expr, data=df0, return_type="dataframe")

# Standardize the design matrix features for better sampling behavior
if 'Intercept' in X_bayes.columns:
    # Scale columns excluding the Intercept
    cols_to_scale = X_bayes.columns.difference(['Intercept'])
    scaler = StandardScaler() # Assuming StandardScaler is already imported
    X_scaled_values = scaler.fit_transform(X_bayes[cols_to_scale])
    X_scaled = pd.DataFrame(X_scaled_values, columns=cols_to_scale, index=X_bayes.index)
    X_scaled['Intercept'] = 1.0 # Re-add the Intercept column as 1s
    # Reorder columns to match original X_bayes order (important for mapping beta priors)
    X_scaled = X_scaled[X_bayes.columns]
else:
     # Scale all columns if no intercept is generated by patsy (less common for regression)
     scaler = StandardScaler()
     X_scaled_values = scaler.fit_transform(X_bayes)
     X_scaled = pd.DataFrame(X_scaled_values, columns=X_bayes.columns, index=X_bayes.index)


# Extract observed outcome (integer LOS)
y_raw   = df0["Post_OP_LOS"].values
y_int   = np.rint(y_raw).astype("int64")
# Replace any negative values resulting from rint (shouldn't happen with Post_OP_LOS >= 0)
y_int[y_int < 0] = 0
assert (y_int >= 0).all() # Ensure all outcome values are non-negative integers

# Define the PyMC model
with pm.Model() as nb_model_pymc:

    # Define priors for the model parameters
    # Intercept: Use a weakly informative Normal prior
    intercept = pm.Normal("intercept", mu=0, sigma=10)
    beta = pm.Normal("beta", mu=0, sigma=2, shape=X_scaled.shape[1])


    # Dispersion parameter (alpha) for Negative Binomial
    # Use a Half-Cauchy prior, common for scale parameters, constrained to be positive
    alpha = pm.HalfCauchy("alpha", beta=10)


    # Define the linear model (log link function for Negative Binomial)
    mu = pm.math.exp(intercept + pm.math.dot(X_scaled, beta))


    # Define the likelihood function (Negative Binomial)
    y_obs = pm.NegativeBinomial("y_obs", mu=mu, alpha=alpha, observed=y_int)


    # Sample from the posterior
    trace = pm.sample(
       draws=2000,
       tune=1000,
       target_accept=0.95,
       cores=2,
       return_inferencedata=True,

    )

# Posterior summary (including intercept, alpha, and beta)
print("\n--- Posterior Summary (intercept, alpha, beta) ---")

idata = trace 
az_summary = az.summary(idata, var_names=["intercept", "alpha", "beta"])

# Rename beta indices to actual feature names
beta_names = X_scaled.columns.tolist()
# Check if 'beta' is in the summary index before renaming
if 'beta' in az_summary.index.get_level_values(0):
    # Create a mapping from default beta index (e.g., beta[0], beta[1]) to feature name
    beta_index_mapping = {f'beta[{i}]': name for i, name in enumerate(beta_names)}
    # Apply the renaming to the MultiIndex
    new_index = [(level1, beta_index_mapping.get(level2, level2)) for level1, level2 in az_summary.index]
    az_summary.index = pd.MultiIndex.from_tuples(new_index, names=az_summary.index.names)


display(az_summary)

# Forest plot of the betas
# Use the InferenceData object and specify var_names
az.plot_forest(idata, var_names=["beta"], combined=True)
plt.title("Posterior of β Coefficients (Bayesian NB)")
plt.tight_layout()
plt.show()

# Note: You might want to save the trace or summary for later use.
az.to_netcdf(idata, "nb_model_pymc_trace.nc")

# The subsequent steps for ZINB comparison and PPC should also be updated
# to use the 'trace' object obtained from this sampling step.
# For example, the PPC step:
with nb_model_pymc: # Use the model defined above
  ppc = pm.sample_posterior_predictive(trace, var_names=["y_obs"], random_seed=42)
y_draws = ppc.posterior_predictive["y_obs"]

# ────────────────────────────────────────────────────────────────────────────────
# Step 9: Posterior Predictive Checks & Individual‐Level Predictions
# ────────────────────────────────────────────────────────────────────────────────

# 1) Draw posterior predictive samples:
with nb_model_pymc:
    # Sample from the posterior predictive distribution of the observed variable y_obs
    # When var_names is not specified and observed is present, it samples from observed variables.
    ppc = pm.sample_posterior_predictive(trace, random_seed=42)

y_pred_samples = ppc.posterior_predictive["y_obs"].values  
# shape will be (n_chain, n_draws_per_chain, n_obs)
# so you might want to reshape:
y_pred_samples = y_pred_samples.reshape(-1, y_pred_samples.shape[-1])


# Now, ppc should contain a key for the observed variable 'y_obs'
y_pred_mean  = y_pred_samples.mean(axis=0)
y_pred_lower = np.percentile(y_pred_samples, 2.5, axis=0)
y_pred_upper = np.percentile(y_pred_samples, 97.5, axis=0)

# Assemble into a DataFrame for “df0” (imputed_datasets[0]); you can merge to original ID if needed:
individual_predictions = pd.DataFrame({
    "ID":               df0.index,                   # make sure index is original patient ID
    "Observed_LOS":     y_int, # Use y_int (integer outcome) which was used in the model
    "Predicted_Mean_LOS": y_pred_mean,
    "CI_Lower_2.5%":    y_pred_lower,
    "CI_Upper_97.5%":   y_pred_upper,
    "Had_P2R":          df0["Had_P2R"].values,
    # Ensure ASA_Grade is converted to string or a plottable type if it's still categorical/numeric code
    "ASA_Grade":        df0["ASA_Grade"].astype(str), 
})

print("\n--- Individual‐Level Posterior Predictions (first 10 rows) ---")
display(individual_predictions.head(10))

# ────────────────────────────────────────────────────────────────────────────────
# Step 10: Visualise Prediction Accuracy by Treatment & Risk Profile
# ────────────────────────────────────────────────────────────────────────────────

import seaborn as sns

# Re-get asa_codes from df0 if needed (assuming df0 is defined)
if 'df0' in globals():
    if pd.api.types.is_categorical_dtype(df0["ASA_Grade"]):
        asa_codes = df0["ASA_Grade"].cat.codes # Get integer codes from the categorical data
    elif pd.api.types.is_numeric_dtype(df0["ASA_Grade"]):
        asa_codes = df0["ASA_Grade"] # Use numeric values directly if not categorical
    else:
        print("Warning: ASA_Grade is not numeric or categorical. Cannot create risk profile with qcut.")
        # Handle the case where ASA_Grade is neither (e.g., skip risk profile plots or adjust)
        individual_predictions["Risk_Profile"] = "Unknown" # Or some other placeholder
        asa_codes = None # Prevent qcut from running if type is unexpected

    if asa_codes is not None and len(asa_codes.dropna()) > 0:
        # Ensure there are enough unique values for qcut
        if len(asa_codes.dropna().unique()) >= 3:
             individual_predictions["Risk_Profile"] = pd.qcut(
                 asa_codes, q=3, labels=["Low", "Medium", "High"], duplicates='drop' # Add duplicates='drop'
             )
        else:
            print("Warning: Not enough unique ASA_Grade values for 3 tertiles. Cannot create risk profile.")
            individual_predictions["Risk_Profile"] = "Insufficient Data" # Or some other placeholder
    elif asa_codes is not None and len(asa_codes.dropna()) == 0:
         print("Warning: All ASA_Grade values are missing. Cannot create risk profile.")
         individual_predictions["Risk_Profile"] = "All Missing"
else:
     print("Warning: df0 not found. Cannot create ASA risk profile.")
     individual_predictions["Risk_Profile"] = "df0 not found"


# (a) Boxplot of predicted LOS by Had_P2R
plt.figure(figsize=(10, 5))
# Ensure Had_P2R column exists in individual_predictions before plotting
if "Had_P2R" in individual_predictions.columns:
    sns.boxplot(
        data = individual_predictions,
        x    = "Had_P2R",
        y    = "Predicted_Mean_LOS",
        palette = ["#D7191C", "#2C7BB6"]
    )
    plt.title("Predicted Mean LOS by Treatment Group")
    plt.xlabel("Had P2R (0=No, 1=Yes)")
    plt.ylabel("Predicted Mean Length of Stay")
    plt.tight_layout()
    plt.show()
else:
    print("Cannot plot Predicted Mean LOS by Had_P2R: 'Had_P2R' column not found in individual_predictions.")


# (b) Boxplot of predicted LOS by ASA risk tertile, colored by Had_P2R
plt.figure(figsize=(10, 5))
# Ensure Risk_Profile and Had_P2R columns exist before plotting
if "Risk_Profile" in individual_predictions.columns and "Had_P2R" in individual_predictions.columns:
    # Ensure Risk_Profile has valid categories for plotting
    if not individual_predictions["Risk_Profile"].empty and individual_predictions["Risk_Profile"].nunique() > 1:
        sns.boxplot(
            data = individual_predictions,
            x    = "Risk_Profile",
            y    = "Predicted_Mean_LOS",
            hue  = "Had_P2R",
            palette = ["#D7191C", "#2C7BB6"]
        )
        plt.title("Predicted LOS by ASA Risk Profile & Treatment")
        plt.xlabel("ASA Risk Profile (Tertiles)")
        plt.ylabel("Predicted Mean Length of Stay")
        plt.legend(title="Had P2R")
        plt.tight_layout()
        plt.show()
    else:
        print("Cannot plot Predicted LOS by ASA Risk Profile: 'Risk_Profile' column is empty or has insufficient unique values.")
else:
    print("Cannot plot Predicted LOS by ASA Risk Profile & Treatment: Required columns ('Risk_Profile' or 'Had_P2R') not found in individual_predictions.")


# Note: You might want to save the trace or summary for later use.
az.to_netcdf(idata, "nb_model_pymc_trace.nc")

df_cf = df_model.copy()

df_cf_treated   = df_cf.assign(Had_P2R=1)
df_cf_untreated = df_cf.assign(Had_P2R=0)

# 2) Get the model’s fitted mean (mu) under each scenario
mu_treated   = nb_model.predict(df_cf_treated)
mu_untreated = nb_model.predict(df_cf_untreated)

# 3) Build cf_df
cf_df = pd.DataFrame({
    "Treated_Mean":   mu_treated,
    "Untreated_Mean": mu_untreated
})


# 2) Counterfactual KDE: everyone treated vs everyone untreated
plt.figure(figsize=(8, 5))
sns.kdeplot(
    cf_df['Treated_Mean'],
    label="All Treated",
    fill=True,
    alpha=0.6
)
sns.kdeplot(
    cf_df['Untreated_Mean'],
    label="All Untreated",
    fill=True,
    alpha=0.6
)
plt.title("Counterfactual Prediction Distributions by Treatment Status")
plt.xlabel("Expected LOS")
plt.legend()
plt.show()


# 3) Posterior of Individual‐Level Treatment Effects
#    (difference between treated and untreated predictions)
diff = cf_df['Treated_Mean'] - cf_df['Untreated_Mean']

plt.figure(figsize=(8, 5))
sns.kdeplot(diff, fill=True, color="#8172B3")
plt.axvline(0, color='grey', linestyle='--', linewidth=1)
plt.title("Posterior Distribution of Individual‐Level Treatment Effects")
plt.xlabel("Δ Expected LOS (Treated − Untreated)")
plt.tight_layout()
plt.show()



# compute per‐patient mean under each scenario
treated_mean   = cf_treated.mean(axis=0)
untreated_mean = cf_untreated.mean(axis=0)
df_cf = df0.assign(
    TreatedMean=mu_treated,      # Use the predicted means directly
    UntreatedMean=mu_untreated   # Use the predicted means directly
)

from sklearn.mixture import GaussianMixture

# Suppose `y_pred_mean` is your vector of predicted mean LOS from the Bayesian model:
y = y_pred_mean.reshape(-1,1)

# fit a 2-component Gaussian mixture on the *predictions*
gm = GaussianMixture(n_components=2, random_state=0).fit(y)
clusters = gm.predict(y)       # 0 or 1

# attach back to your data
df0 = imputed_datasets[0].copy()
df0['cluster'] = clusters

# now you can do:
print(df0.groupby('cluster')[['Age','ASA_Grade','Cancer_Stage']].describe())

# or cross-tabulate
print(pd.crosstab(df0['cluster'], df0['Type_of_Cancer'], normalize='index'))

print("Cluster centers (mean predicted LOS):", gm.means_.ravel())
for var in ["Had_P2R","ASA_Grade","WIMD_Quint"]:
    print(var, pd.crosstab(df0["cluster"], df0[var], normalize="index"))


df_model["cluster"] = gm.predict(X_imp)  # or gm.labels_
formula = (
    "Post_OP_LOS ~ Had_P2R + C(cluster)"
    + " + C(ASA_Grade, Treatment(reference=2))"
    + " + C(Cancer_Stage, Treatment(reference=2))"
    + " + C(Type_of_Cancer)"
    + " + C(WIMD_Quint)"
    + " + C(Sex)"
    + " + Age + Time_Diagnosis_Admission"
)
nb_with_cluster = smf.glm(formula, data=df_model,
                          family=sm.families.NegativeBinomial(alpha=best_alpha),
                          freq_weights=df_model["weights"]).fit()
print(nb_with_cluster.summary())

with pm.Model() as mix_nb:
    n_obs = y_int.shape[0] # Define the number of observations
    π = pm.Dirichlet("π", a=np.array([1,1]))
    α = pm.HalfNormal("α", sigma=2, shape=2)
    β = pm.Normal("β", mu=0, sigma=1, shape=(2, X_scaled.shape[1]))
    intercept = pm.Normal("intercept", mu=0, sigma=5, shape=2)
    # latent cluster assignment
    z = pm.Categorical("z", p=π, shape=n_obs) # Use n_obs here
    mu = pm.math.exp(intercept[z] + (X_scaled.values * β[z]).sum(axis=1))
    y_obs = pm.NegativeBinomial("y_obs", mu=mu, alpha=α[z], observed=y_int) # Corrected observed variable name if necessary
    trace_mix = pm.sample(1000, tune=1000)






