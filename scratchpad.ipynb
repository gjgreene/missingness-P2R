{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gjgreene/missingness-P2R/blob/main/scratchpad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIYdn1woOS1n"
      },
      "outputs": [],
      "source": [
        "%env JAX_PLATFORM_NAME=gpu\n",
        "!pip install numpyro[cuda] -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "!pip install pymc seaborn patsy miceforest openpyxl statsmodels NumPyro\n",
        "!pip install xgboost\n",
        "## exploring the imputation methods for the P2R evaluation\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import miceforest as mf # This import requires miceforest to be installed\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import log_loss\n",
        "import pymc as pm\n",
        "import arviz as az\n",
        "import patsy\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_excel(\"P2R_Full_Data3.xlsx\")\n",
        "\n",
        "# Preview data\n",
        "print(df.head())\n",
        "\n",
        "df.rename(columns={\n",
        "    \"Type of Cancer\": \"Type_of_Cancer\",\n",
        "    \"WIMD_Qunit\": \"WIMD_Quint\"\n",
        "}, inplace=True)\n",
        "\n",
        "# Cast WIMD_Quint as ordered categorical (levels 1-5)\n",
        "df[\"WIMD_Quint\"] = pd.Categorical(\n",
        "    df[\"WIMD_Quint\"],\n",
        "    categories=[1, 2, 3, 4, 5],\n",
        "    ordered=True\n",
        ")\n",
        "\n",
        "# Cast Sex as nominal categorical with labels\n",
        "df[\"Sex\"] = pd.Categorical(\n",
        "    df[\"Sex\"].map({1: \"Male\", 2: \"Female\"}),\n",
        "    categories=[\"Male\", \"Female\"],\n",
        "    ordered=False\n",
        ")\n",
        "\n",
        "# ASA Grade as ordered categorical (levels 1-4)\n",
        "df[\"ASA_Grade\"] = pd.Categorical(\n",
        "    df[\"ASA_Grade\"],\n",
        "    categories=[1, 2, 3, 4],\n",
        "    ordered=True\n",
        ")\n",
        "\n",
        "# Type_of_Cancer as nominal categorical\n",
        "df[\"Type_of_Cancer\"] = pd.Categorical(\n",
        "    df[\"Type_of_Cancer\"].map({1: \"Colorectal\", 2: \"Upper_GI\", 3: \"HPB\"}),\n",
        "    categories=[\"Colorectal\", \"Upper_GI\", \"HPB\"],\n",
        "    ordered=False\n",
        ")\n",
        "\n",
        "# Cancer Stage as ordered categorical (levels 1-4)\n",
        "df[\"Cancer_Stage\"] = pd.Categorical(\n",
        "    df[\"Cancer_Stage\"],\n",
        "    categories=[1, 2, 3, 4],\n",
        "    ordered=True\n",
        ")\n",
        "\n",
        "# Chemotherapy as nominal categorical\n",
        "df[\"Chemotherapy\"] = pd.Categorical(\n",
        "    df[\"Chemotherapy\"].map({0: \"No\", 1: \"Yes\"}),\n",
        "    categories=[\"No\", \"Yes\"],\n",
        "    ordered=False\n",
        ")\n",
        "\n",
        "# Radiotherapy as nominal categorical\n",
        "df[\"Radiotherapy\"] = pd.Categorical(\n",
        "    df[\"Radiotherapy\"].map({0: \"No\", 1: \"Yes\"}),\n",
        "    categories=[\"No\", \"Yes\"],\n",
        "    ordered=False\n",
        ")\n",
        "\n",
        "# Cap unrealistic age values\n",
        "df.loc[df[\"Age\"] > 100, \"Age\"] = np.nan\n",
        "\n",
        "# Create imputation flags for key variables with missingness\n",
        "for col in [\"Cancer_Stage\", \"Chemotherapy\", \"Radiotherapy\"]:\n",
        "    df[f\"{col}_imputed\"] = df[col].isnull().astype(int)\n",
        "\n",
        "# Square of Age for modelling nonlinear effects\n",
        "df[\"Age_sq\"] = df[\"Age\"] ** 2\n",
        "\n",
        "# Define treatment and outcome clearly\n",
        "treatment = \"Had_P2R\"\n",
        "outcome = \"Post_OP_LOS\"\n",
        "\n",
        "# Covariates list for future analysis\n",
        "covariates = [\n",
        "    \"ASA_Grade\", \"Cancer_Stage\", \"Type_of_Cancer\",\n",
        "    \"WIMD_Quint\", \"Time_Diagnosis_Admission\", \"Age\",\n",
        "    \"Sex\", \"Chemotherapy\", \"Radiotherapy\",\n",
        "    \"Cancer_Stage_imputed\", \"Chemotherapy_imputed\", \"Radiotherapy_imputed\"\n",
        "]\n",
        "\n",
        "# Define variable types clearly\n",
        "ordinal_vars = [\"ASA_Grade\", \"Cancer_Stage\"]\n",
        "categorical_vars = [\"Sex\", \"Type_of_Cancer\", \"WIMD_Quint\", \"Chemotherapy\", \"Radiotherapy\"]\n",
        "continuous_vars = [\"Age\", \"Age_sq\", \"Time_Diagnosis_Admission\"]\n",
        "\n",
        "# Confirm structure and categories\n",
        "print(df[covariates].dtypes)\n",
        "\n",
        "\n",
        "# Check missing data summary\n",
        "print(df[covariates].isnull().mean().sort_values(ascending=False))\n",
        "\n",
        "# Quick summary statistics\n",
        "print(df.describe(include='all'))\n",
        "\n",
        "## Section 2: Imputation (MICE & GPU acceleration)\n",
        "\n",
        "# Copy dataset for imputation to avoid modifying original\n",
        "data_for_impute = df[covariates + [treatment, outcome]].copy()\n",
        "\n",
        "# Replace infinite values with NaN\n",
        "data_for_impute.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Store original categories to map back later\n",
        "original_categories = {}\n",
        "categorical_vars_all = ordinal_vars + categorical_vars\n",
        "\n",
        "for col in categorical_vars_all:\n",
        "    original_categories[col] = (df[col].cat.categories, df[col].cat.ordered)\n",
        "    # Convert categorical to numeric codes for imputation\n",
        "    data_for_impute[col] = df[col].cat.codes.replace(-1, np.nan).astype(float)\n",
        "\n",
        "# Number of observed (non-missing) values for TDA\n",
        "n_obs_tda = data_for_impute[\"Time_Diagnosis_Admission\"].notna().sum()\n",
        "\n",
        "# Drop rows with missing predictors (except TDA)\n",
        "tda_predictors = data_for_impute.drop(columns=[\"Time_Diagnosis_Admission\"]).columns\n",
        "clean_data_tda = data_for_impute.dropna(subset=tda_predictors).copy()\n",
        "\n",
        "# PMM for TDA using miceforest\n",
        "tda_kernel = mf.ImputationKernel(\n",
        "    data = data_for_impute.copy(),\n",
        "    random_state = 42,\n",
        "    num_datasets = 20,\n",
        "    save_all_iterations_data = True,\n",
        "    mean_match_candidates={\"Time_Diagnosis_Admission\": n_obs_tda}\n",
        ")\n",
        "\n",
        "# run the chained imputations\n",
        "tda_kernel.mice(iterations=20)\n",
        "\n",
        "# Extract TDA imputations\n",
        "tda_imputed_values = [\n",
        "    tda_kernel.complete_data(m)[\"Time_Diagnosis_Admission\"]\n",
        "    for m in range(20)\n",
        "]\n",
        "\n",
        "\n",
        "# Custom class to add noise for better uncertainty handling\n",
        "class XGBWithNoise(XGBRegressor):\n",
        "    def fit(self, X, y, **kwargs):\n",
        "        super().fit(X, y, **kwargs)\n",
        "        preds = super().predict(X)\n",
        "        self._resid_std = np.std(y - preds, ddof=1)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X, return_std=False):\n",
        "        preds = super().predict(X)\n",
        "        if return_std:\n",
        "            return preds, np.full_like(preds, self._resid_std, dtype=float)\n",
        "        return preds\n",
        "\n",
        "# GPU-based imputer setup\n",
        "gpu_xgb = XGBWithNoise(\n",
        "    tree_method='hist',\n",
        "    device='cuda',\n",
        "    random_state=42,\n",
        "    n_estimators=100,\n",
        "    max_depth=6\n",
        ")\n",
        "\n",
        "imputer = IterativeImputer(\n",
        "    estimator=gpu_xgb,\n",
        "    sample_posterior=True,\n",
        "    max_iter=20,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit imputer (includes TDA but we'll overwrite with PMM results)\n",
        "imputer.fit(data_for_impute.values)\n",
        "\n",
        "imputed_datasets = []\n",
        "\n",
        "for m in range(20):\n",
        "    imputer.random_state = 42 + m\n",
        "    imputed_array = imputer.transform(data_for_impute.values)\n",
        "\n",
        "    df_imputed = pd.DataFrame(imputed_array, columns=data_for_impute.columns)\n",
        "\n",
        "    # Overwrite TDA with PMM-imputed values to preserve original distribution\n",
        "    df_imputed[\"Time_Diagnosis_Admission\"] = tda_imputed_values[m].values\n",
        "\n",
        "    # Restore original categorical variables\n",
        "    for col, (cats, ordered) in original_categories.items():\n",
        "        codes = df_imputed[col].round().astype(int).clip(0, len(cats)-1)\n",
        "        df_imputed[col] = pd.Categorical.from_codes(codes, categories=cats, ordered=ordered)\n",
        "\n",
        "    # Reattach original treatment and outcome\n",
        "    df_imputed[treatment] = df[treatment].values\n",
        "    df_imputed[outcome] = df[outcome].values\n",
        "\n",
        "    imputed_datasets.append(df_imputed)\n",
        "    print(f\"Imputed dataset {m+1} completed\")\n",
        "\n",
        "# Example: Check first imputed dataset\n",
        "print(imputed_datasets[0].info())\n",
        "print(imputed_datasets[0].describe(include='all'))\n",
        "\n",
        "# Verify distribution of TDA against original\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.kdeplot(df[\"Time_Diagnosis_Admission\"].dropna(), label='Original', fill=True)\n",
        "sns.kdeplot(imputed_datasets[0][\"Time_Diagnosis_Admission\"], label='Imputed #1', fill=True)\n",
        "plt.legend()\n",
        "plt.title(\"Distribution of Time_Diagnosis_Admission: Original vs Imputed\")\n",
        "plt.show()\n",
        "\n",
        "ps_covariates = [\n",
        "    \"Age\",\n",
        "    \"Time_Diagnosis_Admission\",\n",
        "    \"Sex\",\n",
        "    \"Type_of_Cancer\",\n",
        "    \"ASA_Grade\",\n",
        "    \"Cancer_Stage\",\n",
        "    \"WIMD_Quint\",\n",
        "    \"Chemotherapy\",\n",
        "    \"Radiotherapy\",\n",
        "    \"Cancer_Stage_imputed\",\n",
        "    \"Chemotherapy_imputed\",\n",
        "    \"Radiotherapy_imputed\"\n",
        "]\n",
        "\n",
        "# Helper Functions for PS and IPTW\n",
        "\n",
        "def build_design_matrix(df, covariates):\n",
        "    cat_cols = [col for col in covariates if df[col].dtype.name == \"category\"]\n",
        "    return pd.get_dummies(df[covariates], columns=cat_cols, drop_first=True, dtype=float)\n",
        "\n",
        "def stabilised_weights(ps, treatment):\n",
        "    p_treatment = treatment.mean()\n",
        "    weights = np.where(treatment == 1, p_treatment / ps, (1 - p_treatment) / (1 - ps))\n",
        "    # Trim extreme weights\n",
        "    lo, hi = np.percentile(weights, [1, 99])\n",
        "    return np.clip(weights, lo, hi)\n",
        "\n",
        "\n",
        "weighted_imputed_datasets = []\n",
        "\n",
        "for idx, df_imp in enumerate(imputed_datasets):\n",
        "    # Build design matrix\n",
        "    X_ps = build_design_matrix(df_imp, ps_covariates)\n",
        "    y_ps = df_imp[treatment].values\n",
        "\n",
        "    # Standardise predictors\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_ps)\n",
        "\n",
        "    # Logistic regression (hyperparameter tuning can be added later)\n",
        "    model_ps = LogisticRegression(max_iter=5000, solver='liblinear')\n",
        "    model_ps.fit(X_scaled, y_ps)\n",
        "\n",
        "    # Predict propensity scores\n",
        "    ps_scores = model_ps.predict_proba(X_scaled)[:, 1]\n",
        "\n",
        "    # Calculate stabilised IPTW\n",
        "    df_imp['ps'] = ps_scores\n",
        "    df_imp['iptw'] = stabilised_weights(ps_scores, y_ps)\n",
        "\n",
        "    weighted_imputed_datasets.append(df_imp)\n",
        "\n",
        "    print(f\"Calculated PS and IPTW for imputed dataset {idx + 1}\")\n",
        "\n",
        "def compute_smd(df, covariates, treatment, weights=None):\n",
        "    smd_dict = {}\n",
        "    for var in covariates:\n",
        "        if df[var].dtype.name == 'category':\n",
        "            levels = df[var].cat.categories\n",
        "            for lvl in levels:\n",
        "                mask = (df[var] == lvl).astype(int)\n",
        "                mean_treated = np.average(mask[df[treatment] == 1], weights=weights[df[treatment] == 1] if weights is not None else None)\n",
        "                mean_control = np.average(mask[df[treatment] == 0], weights=weights[df[treatment] == 0] if weights is not None else None)\n",
        "                var_pooled = (mean_treated * (1 - mean_treated) + mean_control * (1 - mean_control)) / 2\n",
        "                smd = np.abs(mean_treated - mean_control) / np.sqrt(var_pooled)\n",
        "                smd_dict[f\"{var}_{lvl}\"] = smd\n",
        "        else:\n",
        "            mean_treated = np.average(df[var][df[treatment] == 1], weights=weights[df[treatment] == 1] if weights is not None else None)\n",
        "            mean_control = np.average(df[var][df[treatment] == 0], weights=weights[df[treatment] == 0] if weights is not None else None)\n",
        "            var_pooled = (np.var(df[var][df[treatment] == 1], ddof=1) + np.var(df[var][df[treatment] == 0], ddof=1)) / 2\n",
        "            smd = np.abs(mean_treated - mean_control) / np.sqrt(var_pooled)\n",
        "            smd_dict[var] = smd\n",
        "    return smd_dict\n",
        "\n",
        "# calculate weighted and unweighted SMD for each imputed dataset, then pool\n",
        "smd_unweighted = []\n",
        "smd_weighted = []\n",
        "\n",
        "for df_imp in weighted_imputed_datasets:\n",
        "    smd_unweighted.append(compute_smd(df_imp, covariates, treatment, weights=None))\n",
        "    smd_weighted.append(compute_smd(df_imp, covariates, treatment, weights=df_imp['iptw']))\n",
        "\n",
        "smd_unweighted = pd.DataFrame(smd_unweighted)\n",
        "smd_weighted = pd.DataFrame(smd_weighted)\n",
        "\n",
        "# Calculate the mean SMD across all imputed datasets\n",
        "mean_smd_unweighted = smd_unweighted.mean()\n",
        "mean_smd_weighted = smd_weighted.mean()\n",
        "\n",
        "def plot_love_plot(smd_unweighted, smd_weighted):\n",
        "    # Access index for labels (covariate names/levels)\n",
        "    labels = list(smd_unweighted.index)\n",
        "    # Access values attribute for the SMD values\n",
        "    unweighted = list(smd_unweighted.values)\n",
        "    weighted = list(smd_weighted.values)\n",
        "\n",
        "    plt.figure(figsize=(8, len(labels) * 0.4))\n",
        "    plt.hlines(labels, 0, unweighted, color='red', label='Unweighted', lw=2)\n",
        "    plt.hlines(labels, 0, weighted, color='blue', label='Weighted', lw=2)\n",
        "    plt.axvline(0.1, color='gray', linestyle='--')\n",
        "    plt.axvline(0.2, color='black', linestyle='--')\n",
        "    plt.xlabel(\"Standardised Mean Difference\")\n",
        "    plt.title(\"Covariate Balance (Love Plot)\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_love_plot(mean_smd_unweighted, mean_smd_weighted)\n",
        "\n",
        "\n",
        "### Implementing a Negative Binomial Mixture Model\n",
        "'''\n",
        "# Select the first imputed dataset as an example\n",
        "df_mixture = weighted_imputed_datasets[0].copy()\n",
        "\n",
        "# Outcome variable (LOS), rounded to integers (required for NegBin)\n",
        "y = df_mixture[\"Post_OP_LOS\"].round().astype(int).values\n",
        "\n",
        "# Define formula explicitly\n",
        "formula = (\n",
        "    \"Had_P2R + C(ASA_Grade) + C(Cancer_Stage) + \"\n",
        "    \"C(Type_of_Cancer) + C(WIMD_Quint) + C(Sex) + \"\n",
        "    \"C(Chemotherapy) + C(Radiotherapy) + Age + Time_Diagnosis_Admission\"\n",
        ")\n",
        "\n",
        "# Generate design matrix\n",
        "X_design = patsy.dmatrix(formula, data=df_mixture, return_type='dataframe')\n",
        "X_values = X_design.values\n",
        "\n",
        "with pm.Model() as nb_mix_model:\n",
        "    # Number of observations\n",
        "    n = len(y)\n",
        "\n",
        "    # Mixture component weights (two groups)\n",
        "    weights = pm.Dirichlet('weights', a=np.array([1, 1]))\n",
        "\n",
        "    # Component-specific intercepts\n",
        "    intercepts = pm.Normal('intercepts', mu=0, sigma=10, shape=2)\n",
        "\n",
        "    # Component-specific regression coefficients\n",
        "    coefs = pm.Normal('coefs', mu=0, sigma=2, shape=(2, X_values.shape[1]))\n",
        "\n",
        "    # Component-specific dispersion parameters\n",
        "    alphas = pm.HalfCauchy('alphas', beta=2, shape=2)\n",
        "\n",
        "    # Linear combination and expected LOS (mu) for each mixture component\n",
        "    mu_components = pm.math.exp(intercepts[:, None] + pm.math.dot(coefs, X_values.T))\n",
        "\n",
        "    # Mixture Negative Binomial Likelihood\n",
        "    y_obs = pm.Mixture(\n",
        "        'y_obs',\n",
        "        w=weights,\n",
        "        comp_dists=[\n",
        "            pm.NegativeBinomial.dist(mu=mu_components[0], alpha=alphas[0]),\n",
        "            pm.NegativeBinomial.dist(mu=mu_components[1], alpha=alphas[1])\n",
        "        ],\n",
        "        observed=y\n",
        "    )\n",
        "\n",
        "    # Sample from the posterior\n",
        "    trace_mix = pm.sample(draws=2000, tune=2000, target_accept=0.95, random_seed=42)\n",
        "\n",
        "az.plot_trace(trace_mix, var_names=['weights', 'intercepts', 'alphas'])\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "az.summary(trace_mix, var_names=['weights', 'intercepts', 'alphas'])\n",
        "\n",
        "with nb_mix_model:\n",
        "    ppc = pm.sample_posterior_predictive(trace_mix, random_seed=42)\n",
        "\n",
        "y_pred = ppc.posterior_predictive['y_obs'].stack(draws=(\"chain\", \"draw\")).values\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.kdeplot(y, label=\"Observed LOS\", fill=True, color=\"gray\")\n",
        "sns.kdeplot(y_pred.mean(axis=0), label=\"Predicted LOS\", fill=True, color=\"blue\", alpha=0.6)\n",
        "plt.legend()\n",
        "plt.title(\"Posterior Predictive Check for Mixture Model\")\n",
        "plt.xlabel(\"Length of Stay\")\n",
        "plt.show()\n",
        "\n",
        "weights_summary = az.summary(trace_mix, var_names=['weights'])\n",
        "print(weights_summary)\n",
        "'''\n",
        "\n",
        "import pymc as pm\n",
        "import arviz as az\n",
        "import pytensor.tensor as pt               # PyMC ≥5 uses pytensor\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def build_X(df, formula):\n",
        "    import patsy\n",
        "    X_df = patsy.dmatrix(formula, df, return_type=\"dataframe\")\n",
        "    return X_df, list(X_df.columns)\n",
        "\n",
        "def fit_mix_nb_pymc(df_imp, formula, rng_seed=42):\n",
        "    y = df_imp[\"Post_OP_LOS\"].round().astype(int).values\n",
        "    X_df, colnames = build_X(df_imp, formula)\n",
        "    X = X_df.values\n",
        "    n_obs = y.size\n",
        "\n",
        "    with pm.Model() as model:\n",
        "        w     = pm.Dirichlet(\"w\", a=np.ones(2))\n",
        "        beta0 = pm.Normal(\"beta0\",  0, 10, shape=2)\n",
        "        beta  = pm.Normal(\"beta\",   0,  2, shape=(2, X.shape[1]))\n",
        "        alpha = pm.HalfCauchy(\"α\",  2,       shape=2)\n",
        "\n",
        "        mu = pt.exp(beta0[:, None] + beta.dot(X.T))     # (2, n_obs)\n",
        "\n",
        "        comp = [\n",
        "            pm.NegativeBinomial.dist(mu=mu[0], alpha=alpha[0]),\n",
        "            pm.NegativeBinomial.dist(mu=mu[1], alpha=alpha[1])\n",
        "        ]\n",
        "\n",
        "        pm.Mixture(\"y_like\", w=w, comp_dists=comp, observed=y)\n",
        "\n",
        "        idata = pm.sample(draws=2000, tune=1000,\n",
        "                          chains=2, target_accept=0.9,\n",
        "                          random_seed=rng_seed,\n",
        "                          init=\"jitter+adapt_diag\")\n",
        "    return idata, colnames\n",
        "\n",
        "import os\n",
        "\n",
        "# Define formula explicitly\n",
        "formula = (\n",
        "    \"Had_P2R + C(ASA_Grade) + C(Cancer_Stage) + \"\n",
        "    \"C(Type_of_Cancer) + C(WIMD_Quint) + C(Sex) + \"\n",
        "    \"C(Chemotherapy) + C(Radiotherapy) + Age + Time_Diagnosis_Admission\"\n",
        ")\n",
        "\n",
        "posterior_list = []\n",
        "for m, df_imp in enumerate(weighted_imputed_datasets):\n",
        "    idata, colnames = fit_mix_nb_pymc(df_imp, formula, rng_seed=2025 + m)\n",
        "    posterior_list.append(idata.posterior)\n",
        "    print(f\"✓ finished imputation {m+1}/20\")\n",
        "\n",
        "combined_posterior = az.concat(posterior_list, dim=\"draw\")\n",
        "print(az.summary(combined_posterior, var_names=[\"beta\"], filter_vars=\"like\")\n",
        "        .loc[:, [\"mean\", \"hdi_3%\", \"hdi_97%\"]])\n",
        "\n",
        "\n",
        "# quick summary of the treatment effect coefficients\n",
        "az.summary(combined_posterior,\n",
        "           var_names=[\"beta\"], filter_vars=\"like\").loc[:, [\"mean\", \"hdi_3%\", \"hdi_97%\"]]\n",
        "\n",
        "\n",
        "import arviz as az\n",
        "import pandas as pd\n",
        "from google.colab import files      # ⇠ provides files.download()\n",
        "\n",
        "# ⬛ 1. create the coefficient table ---------------------------------------\n",
        "summary = (\n",
        "    az.summary(\n",
        "        combined_posterior,\n",
        "        var_names=[\"beta\"],          # only regression coefficients\n",
        "        filter_vars=\"like\",\n",
        "        round_to=4\n",
        "    )[[\"mean\", \"hdi_3%\", \"hdi_97%\"]]\n",
        "    .reset_index()\n",
        "    .rename(columns={\n",
        "        \"index\":    \"parameter\",\n",
        "        \"mean\":     \"posterior_mean\",\n",
        "        \"hdi_3%\":   \"lower_95\",\n",
        "        \"hdi_97%\":  \"upper_95\"\n",
        "    })\n",
        ")\n",
        "\n",
        "# ⬛ 2. save to Excel inside Colab’s scratch disk ---------------------------\n",
        "out_path = \"/mnt/data/mixture_nb_coefficients.xlsx\"\n",
        "summary.to_excel(out_path, index=False)\n",
        "print(f\"Excel file saved to {out_path}\")\n",
        "\n",
        "# ⬛ 3. download the file to your computer ---------------------------------\n",
        "files.download(out_path)             # a browser download dialogue pops up\n",
        "\n",
        "# ⬛ 4. (optional) preview in-notebook -------------------------------------\n",
        "import ace_tools as tools\n",
        "tools.display_dataframe_to_user(name=\"Mixture NB coefficients\", dataframe=summary)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}